{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3dd056f-5b81-4e0c-a7bc-d3b39fba0960",
   "metadata": {},
   "source": [
    "# Vector Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f433e9b5-98dd-4c9a-b002-0cb2bdeeb966",
   "metadata": {},
   "source": [
    "## Tokenization with split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4fd84f-f01d-4ade-9a7c-ef3ab3163d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1987.', 'Heritage', 'Monticello', 'Site', 'UNESCO', 'World', 'as', 'designated', 'until', \"wasn't\"]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sentence = \"\"\"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\"\"\n",
    "token_sequence = str.split(sentence)\n",
    "vocab = sorted(set(token_sequence))\n",
    "', '.join(vocab)\n",
    "print(vocab)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f01863-991b-45b7-921c-4d459527e029",
   "metadata": {},
   "source": [
    "## One-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b6d314-ac7c-4a27-a42c-85c8687cea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "[[0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(token_sequence)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(num_tokens)\n",
    "print(vocab_size)\n",
    "\n",
    "onehot_vectors = np.zeros((num_tokens,vocab_size), int)\n",
    "\n",
    "for i, word in enumerate(token_sequence):\n",
    "    onehot_vectors[i, vocab.index(word)] = 1\n",
    "    ' '.join(vocab) \n",
    "    \n",
    "print(onehot_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4264809-33ed-4e30-8362-8fe5dc0ff65b",
   "metadata": {},
   "source": [
    "## Showing One-hot with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf55ccb-3ccf-42d1-8ddc-aaad9f03a1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1987.  Heritage  Monticello  Site  UNESCO  World  as  designated  until  \\\n",
      "0      0         0           1     0       0      0   0           0      0   \n",
      "1      0         0           0     0       0      0   0           0      0   \n",
      "2      0         0           0     0       0      0   0           1      0   \n",
      "3      0         0           0     0       0      0   1           0      0   \n",
      "4      0         0           0     0       1      0   0           0      0   \n",
      "5      0         0           0     0       0      1   0           0      0   \n",
      "6      0         1           0     0       0      0   0           0      0   \n",
      "7      0         0           0     1       0      0   0           0      0   \n",
      "8      0         0           0     0       0      0   0           0      1   \n",
      "9      1         0           0     0       0      0   0           0      0   \n",
      "\n",
      "   wasn't  \n",
      "0       0  \n",
      "1       1  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n",
      "5       0  \n",
      "6       0  \n",
      "7       0  \n",
      "8       0  \n",
      "9       0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.DataFrame(onehot_vectors, columns = vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae92527b-9e97-4d47-ad5e-c4650da4d219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92b79a-823b-48b0-a484-abab65243bfc",
   "metadata": {},
   "source": [
    "# Bag of words (Count Vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85528176-09bc-40d5-bc0b-8730b824d8a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 5)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
       "       'this'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X)\n",
    "\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915509d2-673a-4771-9242-b0b1f75137e9",
   "metadata": {},
   "source": [
    "# Vector similiraty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4e1aa6d-cde1-4dda-a3b0-d288f1f72801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.79056942, 0.54772256, 1.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(X[0:1], X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
